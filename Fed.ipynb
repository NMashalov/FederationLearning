{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzloZJCiZeSnbDW0ArNFaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NMashalov/FederationLearning/blob/master/Fed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code was highly inspired by implementation in paper's repository:\n",
        "https://github.com/hongliny/FedAc-NeurIPS20/tree/master\n",
        "\n",
        "\n",
        "a9a was presented in article\n",
        "https://arxiv.org/abs/2006.08950"
      ],
      "metadata": {
        "id": "DIPpmp5EANwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96_Hakrr8iQ9",
        "outputId": "40820b8d-c5cb-4af0-eaa6-197ca140e08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-15 21:45:39--  https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2329875 (2.2M)\n",
            "Saving to: ‘a9a’\n",
            "\n",
            "a9a                 100%[===================>]   2.22M  1.56MB/s    in 1.4s    \n",
            "\n",
            "2023-11-15 21:45:41 (1.56 MB/s) - ‘a9a’ saved [2329875/2329875]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset loading"
      ],
      "metadata": {
        "id": "NmeFJC5yAOax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "X_train, y_train = load_svmlight_file('a9a')\n",
        "\n",
        "X_train = X_train.toarray()"
      ],
      "metadata": {
        "id": "6i2ixcuk8pF-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broadcast_avg(self, pool):\n",
        "        \"\"\"\n",
        "        Helper functions for FedAc and FedAvg, average and broadcast the weights.\n",
        "        \"\"\"\n",
        "        avg = pool.mean(axis=0)\n",
        "        pool = np.repeat(avg[np.newaxis, :], pool.shape[0], axis=0)\n",
        "        return pool"
      ],
      "metadata": {
        "id": "0H14LGbWBoMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sample_grad(self, weight, batch_size):\n",
        "        \"\"\"\n",
        "        Sample minibatch gradients with size batch_size (For MB-SGD and MB-AC-SGD)\n",
        "\n",
        "        Arguments:\n",
        "            weight: w\n",
        "            batch_size: number of samples queried to compute the sample gradient.\n",
        "\n",
        "        Return:\n",
        "            Averaged gradients\n",
        "        \"\"\"\n",
        "        samples_idx = np.random.choice(self.n_samples, batch_size, replace=True)\n",
        "        X_sampled = self.X_train[samples_idx, :]\n",
        "        y_sampled = self.y_train[samples_idx]\n",
        "\n",
        "        grad = -((y_sampled * (1 - sigm((X_sampled @ weight) * y_sampled))).T @ X_sampled) / \\\n",
        "            len(y_sampled) + self.lambd * weight\n",
        "\n",
        "        return grad"
      ],
      "metadata": {
        "id": "oxSMZOixAEYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fedavg(self, eta, M, K, T, local_batch, record_intvl=512, print_intvl=8192, SEED=0):\n",
        "        \"\"\"\n",
        "        Simulate Federated Averaging (FedAvg, a.k.a. Local-SGD, or Parallel SGD, etc.)\n",
        "\n",
        "        Arguments:\n",
        "            eta:    learning rate\n",
        "            M:      number of workers\n",
        "            K:      synchronization interval, (i.e., local steps)\n",
        "            T:      total parallel runtime\n",
        "            record_intvl:   compute the population loss every record_intvl steps.\n",
        "\n",
        "        Return:\n",
        "            A pandas.Series object of population loss evaluated.\n",
        "        \"\"\"\n",
        "        np.random.seed(SEED)\n",
        "        common_init_w = np.random.randn(*self.weight_shape)\n",
        "        w_pool = np.repeat(common_init_w[np.newaxis, :], M, axis=0)\n",
        "\n",
        "        seq = pd.Series(name='loss')\n",
        "        for iter_cnt in range(T+1):\n",
        "            if iter_cnt % K == 0:\n",
        "                w_pool = self.broadcast_avg(w_pool)\n",
        "\n",
        "                if iter_cnt % record_intvl == 0:\n",
        "                    seq.at[iter_cnt] = self.population_loss(w_pool[0, :])\n",
        "\n",
        "            w_pool -= eta * self.sample_grad_pool(w_pool, M, local_batch)\n",
        "        return seq"
      ],
      "metadata": {
        "id": "MasgYJCbBb0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def fedac(self, eta, gamma, alpha, beta, M, K, T, local_batch, record_intvl=512, print_intvl=8192, SEED=0):\n",
        "        \"\"\"\n",
        "        Simulate FedAc\n",
        "\n",
        "        Arguments:\n",
        "            eta:    learning rate\n",
        "            gamma, alpha, beta:  hyperparameters\n",
        "            M:      number of workers\n",
        "            K:      synchronization interval\n",
        "            T:      total parallel runtime\n",
        "            record_intvl:   compute the population loss every record_intvl steps.\n",
        "\n",
        "        Return:\n",
        "            A pandas.Series object of population loss evaluated.\n",
        "        \"\"\"\n",
        "        np.random.seed(SEED)\n",
        "        common_init_w = np.random.randn(*self.weight_shape)\n",
        "        w_pool = np.repeat(common_init_w[np.newaxis, :], M, axis=0)\n",
        "        w_ag_pool = np.copy(w_pool)\n",
        "\n",
        "        seq = pd.Series(name='loss')\n",
        "\n",
        "        for iter_cnt in range(T+1):\n",
        "            if iter_cnt % K == 0:\n",
        "                w_pool = self.broadcast_avg(w_pool)\n",
        "                w_ag_pool = self.broadcast_avg(w_ag_pool)\n",
        "\n",
        "                if iter_cnt % record_intvl == 0:\n",
        "                    seq.at[iter_cnt] = self.population_loss(w_ag_pool[0, :])\n",
        "\n",
        "            w_md_pool = (1/beta) * w_pool + (1-(1/beta))*w_ag_pool\n",
        "            grad_md_pool = self.sample_grad_pool(w_md_pool, M, local_batch)\n",
        "            w_ag_pool = w_md_pool - eta * grad_md_pool\n",
        "            w_pool = (1 - (1/alpha)) * w_pool + (1/alpha) * \\\n",
        "                w_md_pool - gamma * grad_md_pool\n"
      ],
      "metadata": {
        "id": "sJK_PQwCBgBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(self, X, Y, weight):\n",
        "        \"\"\"\n",
        "        Compute the loss.\n",
        "        \"\"\"\n",
        "        return -np.mean(np.log(sigm((X @ weight) * Y))) + 0.5 * self.lambd * np.linalg.norm(weight) ** 2"
      ],
      "metadata": {
        "id": "trGv9TcE_-9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = pd.Series(name='loss')\n",
        "    for iter_cnt in range(0, T+1, K):\n",
        "        if iter_cnt % record_intvl == 0:\n",
        "            seq.at[iter_cnt] = self.population_loss(w)\n",
        "        w -= eta * self.sample_grad(w, M*K*local_batch)\n",
        "    return seq"
      ],
      "metadata": {
        "id": "DuHD9OXf9ID7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}